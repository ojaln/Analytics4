{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def correct_dimensions(s, targetlength):\n",
    "    \"\"\"checks the dimensionality of some numeric argument s, broadcasts it\n",
    "       to the specified length if possible.\n",
    "\n",
    "    Args:\n",
    "        s: None, scalar or 1D array\n",
    "        targetlength: expected length of s\n",
    "\n",
    "    Returns:\n",
    "        None if s is None, else numpy vector of length targetlength\n",
    "    \"\"\"\n",
    "    if s is not None:\n",
    "        s = np.array(s)\n",
    "        if s.ndim == 0:\n",
    "            s = np.array([s] * targetlength)\n",
    "        elif s.ndim == 1:\n",
    "            if not len(s) == targetlength:\n",
    "                raise ValueError(\"arg must have length \" + str(targetlength))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class ESN():\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, n_reservoir=200,\n",
    "                 spectral_radius=0.95, sparsity=0, noise=0.001, input_shift=None,\n",
    "                 input_scaling=None, teacher_forcing=True, feedback_scaling=None,\n",
    "                 teacher_scaling=None, teacher_shift=None,\n",
    "                 out_activation=identity, inverse_out_activation=identity,\n",
    "                 random_state=None, silent=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_inputs: nr of input dimensions\n",
    "            n_outputs: nr of output dimensions\n",
    "            n_reservoir: nr of reservoir neurons\n",
    "            spectral_radius: spectral radius of the recurrent weight matrix\n",
    "            sparsity: proportion of recurrent weights set to zero\n",
    "            noise: noise added to each neuron (regularization)\n",
    "            input_shift: scalar or vector of length n_inputs to add to each\n",
    "                        input dimension before feeding it to the network.\n",
    "            input_scaling: scalar or vector of length n_inputs to multiply\n",
    "                        with each input dimension before feeding it to the netw.\n",
    "            teacher_forcing: if True, feed the target back into output units\n",
    "            teacher_scaling: factor applied to the target signal\n",
    "            teacher_shift: additive term applied to the target signal\n",
    "            out_activation: output activation function (applied to the readout)\n",
    "            inverse_out_activation: inverse of the output activation function\n",
    "            random_state: positive integer seed, np.rand.RandomState object,\n",
    "                          or None to use numpy's builting RandomState.\n",
    "            silent: supress messages\n",
    "        \"\"\"\n",
    "        # check for proper dimensionality of all arguments and write them down.\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_reservoir = n_reservoir\n",
    "        self.n_outputs = n_outputs\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "        self.noise = noise\n",
    "        self.input_shift = correct_dimensions(input_shift, n_inputs)\n",
    "        self.input_scaling = correct_dimensions(input_scaling, n_inputs)\n",
    "\n",
    "        self.teacher_scaling = teacher_scaling\n",
    "        self.teacher_shift = teacher_shift\n",
    "\n",
    "        self.out_activation = out_activation\n",
    "        self.inverse_out_activation = inverse_out_activation\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # the given random_state might be either an actual RandomState object,\n",
    "        # a seed or None (in which case we use numpy's builtin RandomState)\n",
    "        if isinstance(random_state, np.random.RandomState):\n",
    "            self.random_state_ = random_state\n",
    "        elif random_state:\n",
    "            try:\n",
    "                self.random_state_ = np.random.RandomState(random_state)\n",
    "            except TypeError as e:\n",
    "                raise Exception(\"Invalid seed: \" + str(e))\n",
    "        else:\n",
    "            self.random_state_ = np.random.mtrand._rand\n",
    "\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.silent = silent\n",
    "        self.initweights()\n",
    "\n",
    "    def initweights(self):\n",
    "        # initialize recurrent weights:\n",
    "        # begin with a random matrix centered around zero:\n",
    "        W = self.random_state_.rand(self.n_reservoir, self.n_reservoir) - 0.5\n",
    "        # delete the fraction of connections given by (self.sparsity):\n",
    "        W[self.random_state_.rand(*W.shape) < self.sparsity] = 0\n",
    "        # compute the spectral radius of these weights:\n",
    "        radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "        # rescale them to reach the requested spectral radius:\n",
    "        self.W = W * (self.spectral_radius / radius)\n",
    "\n",
    "        # random input weights:\n",
    "        self.W_in = self.random_state_.rand(\n",
    "            self.n_reservoir, self.n_inputs) * 2 - 1\n",
    "        # random feedback (teacher forcing) weights:\n",
    "        self.W_feedb = self.random_state_.rand(\n",
    "            self.n_reservoir, self.n_outputs) * 2 - 1\n",
    "\n",
    "    def _update(self, state, input_pattern, output_pattern):\n",
    "        \"\"\"performs one update step.\n",
    "\n",
    "        i.e., computes the next network state by applying the recurrent weights\n",
    "        to the last state & and feeding in the current input and output patterns\n",
    "        \"\"\"\n",
    "        if self.teacher_forcing:\n",
    "            preactivation = (np.dot(self.W, state)\n",
    "                             + np.dot(self.W_in, input_pattern)\n",
    "                             + np.dot(self.W_feedb, output_pattern))\n",
    "        else:\n",
    "            preactivation = (np.dot(self.W, state)\n",
    "                             + np.dot(self.W_in, input_pattern))\n",
    "        return (np.tanh(preactivation)\n",
    "                + self.noise * (self.random_state_.rand(self.n_reservoir) - 0.5))\n",
    "\n",
    "    def _scale_inputs(self, inputs):\n",
    "        \"\"\"for each input dimension j: multiplies by the j'th entry in the\n",
    "        input_scaling argument, then adds the j'th entry of the input_shift\n",
    "        argument.\"\"\"\n",
    "        if self.input_scaling is not None:\n",
    "            inputs = np.dot(inputs, np.diag(self.input_scaling))\n",
    "        if self.input_shift is not None:\n",
    "            inputs = inputs + self.input_shift\n",
    "        return inputs\n",
    "\n",
    "    def _scale_teacher(self, teacher):\n",
    "        \"\"\"multiplies the teacher/target signal by the teacher_scaling argument,\n",
    "        then adds the teacher_shift argument to it.\"\"\"\n",
    "        if self.teacher_scaling is not None:\n",
    "            teacher = teacher * self.teacher_scaling\n",
    "        if self.teacher_shift is not None:\n",
    "            teacher = teacher + self.teacher_shift\n",
    "        return teacher\n",
    "\n",
    "    def _unscale_teacher(self, teacher_scaled):\n",
    "        \"\"\"inverse operation of the _scale_teacher method.\"\"\"\n",
    "        if self.teacher_shift is not None:\n",
    "            teacher_scaled = teacher_scaled - self.teacher_shift\n",
    "        if self.teacher_scaling is not None:\n",
    "            teacher_scaled = teacher_scaled / self.teacher_scaling\n",
    "        return teacher_scaled\n",
    "\n",
    "    def fit(self, inputs, outputs, inspect=False):\n",
    "        \"\"\"\n",
    "        Collect the network's reaction to training data, train readout weights.\n",
    "\n",
    "        Args:\n",
    "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
    "            outputs: array of dimension (N_training_samples x n_outputs)\n",
    "            inspect: show a visualisation of the collected reservoir states\n",
    "\n",
    "        Returns:\n",
    "            the network's output on the training data, using the trained weights\n",
    "        \"\"\"\n",
    "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
    "        if inputs.ndim < 2:\n",
    "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
    "        if outputs.ndim < 2:\n",
    "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
    "        # transform input and teacher signal:\n",
    "        inputs_scaled = self._scale_inputs(inputs)\n",
    "        teachers_scaled = self._scale_teacher(outputs)\n",
    "\n",
    "        if not self.silent:\n",
    "            print(\"harvesting states...\")\n",
    "        # step the reservoir through the given input,output pairs:\n",
    "        states = np.zeros((inputs.shape[0], self.n_reservoir))\n",
    "        for n in range(1, inputs.shape[0]):\n",
    "            states[n, :] = self._update(states[n - 1], inputs_scaled[n, :],\n",
    "                                        teachers_scaled[n - 1, :])\n",
    "\n",
    "        # learn the weights, i.e. find the linear combination of collected\n",
    "        # network states that is closest to the target output\n",
    "        if not self.silent:\n",
    "            print(\"fitting...\")\n",
    "        # we'll disregard the first few states:\n",
    "        transient = min(int(inputs.shape[1] / 10), 100)\n",
    "        # include the raw inputs:\n",
    "        extended_states = np.hstack((states, inputs_scaled))\n",
    "        # Solve for W_out:\n",
    "        self.W_out = np.dot(np.linalg.pinv(extended_states[transient:, :]),\n",
    "                            self.inverse_out_activation(teachers_scaled[transient:, :])).T\n",
    "\n",
    "        # remember the last state for later:\n",
    "        self.laststate = states[-1, :]\n",
    "        self.lastinput = inputs[-1, :]\n",
    "        self.lastoutput = teachers_scaled[-1, :]\n",
    "\n",
    "        # optionally visualize the collected states\n",
    "        if inspect:\n",
    "            from matplotlib import pyplot as plt\n",
    "            # (^-- we depend on matplotlib only if this option is used)\n",
    "            plt.figure(\n",
    "                figsize=(states.shape[0] * 0.0025, states.shape[1] * 0.01))\n",
    "            plt.imshow(extended_states.T, aspect='auto',\n",
    "                       interpolation='nearest')\n",
    "            plt.colorbar()\n",
    "\n",
    "        if not self.silent:\n",
    "            print(\"training error:\")\n",
    "        # apply learned weights to the collected states:\n",
    "        pred_train = self._unscale_teacher(self.out_activation(\n",
    "            np.dot(extended_states, self.W_out.T)))\n",
    "        if not self.silent:\n",
    "            print(np.sqrt(np.mean((pred_train - outputs)**2)))\n",
    "        return pred_train\n",
    "\n",
    "    def predict(self, inputs, continuation=True):\n",
    "        \"\"\"\n",
    "        Apply the learned weights to the network's reactions to new input.\n",
    "\n",
    "        Args:\n",
    "            inputs: array of dimensions (N_test_samples x n_inputs)\n",
    "            continuation: if True, start the network from the last training state\n",
    "\n",
    "        Returns:\n",
    "            Array of output activations\n",
    "        \"\"\"\n",
    "        if inputs.ndim < 2:\n",
    "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        if continuation:\n",
    "            laststate = self.laststate\n",
    "            lastinput = self.lastinput\n",
    "            lastoutput = self.lastoutput\n",
    "        else:\n",
    "            laststate = np.zeros(self.n_reservoir)\n",
    "            lastinput = np.zeros(self.n_inputs)\n",
    "            lastoutput = np.zeros(self.n_outputs)\n",
    "\n",
    "        inputs = np.vstack([lastinput, self._scale_inputs(inputs)])\n",
    "        states = np.vstack(\n",
    "            [laststate, np.zeros((n_samples, self.n_reservoir))])\n",
    "        outputs = np.vstack(\n",
    "            [lastoutput, np.zeros((n_samples, self.n_outputs))])\n",
    "\n",
    "        for n in range(n_samples):\n",
    "            states[\n",
    "                n + 1, :] = self._update(states[n, :], inputs[n + 1, :], outputs[n, :])\n",
    "            outputs[n + 1, :] = self.out_activation(np.dot(self.W_out,\n",
    "                                                           np.concatenate([states[n + 1, :], inputs[n + 1, :]])))\n",
    "\n",
    "        return self._unscale_teacher(self.out_activation(outputs[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
